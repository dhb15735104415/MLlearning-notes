{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### 线性回归中使用梯度下降法\n",
    "\n",
    "线性回归中,为了是模型更好,需要使损失函数尽可能下\n",
    "\n",
    "在之前的学习中,求出系数和截距值即可\n",
    "\n",
    "本案例中,线性回归中使用梯度下降法来求解系数和截距  "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "X.shape: (100, 1)\n",
      "y.shape: (100,)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# 构造数据\n",
    "np.random.seed(100)  # 设定随机种子,方便多次测试\n",
    "x = 2* np.random.random(size=100)\n",
    "y = x*3. + 4 + np.random.normal(size=100)\n",
    "\n",
    "X = x.reshape(-1, 1)\n",
    "print('X.shape:',X.shape)\n",
    "print('y.shape:',y.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 绘制图像\n",
    "plt.scatter(x, y)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "线性回归中损失函数公式:\n",
    "\n",
    "[均方误差公式](img/均方误差MSE.png)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# 定义损失函数\n",
    "\n",
    "def J(theta, X_b, y):\n",
    "    \"\"\"\n",
    "        损失函数定义\n",
    "    :theta 求导变量,关于theta求导\n",
    "    :X_b 数据集X新增一列,且值都为1的新矩阵\n",
    "    :y   数据集y\n",
    "    :return 损失函数,如值太大无法表示,则返回 float的最大值\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return np.sum((y - X_b.dot(theta)) ** 2) / len(X_b)\n",
    "    except:\n",
    "        return float('inf')\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "损失函数求导公式\n",
    "\n",
    "[损失函数求导公式](img/线性回归使用梯度下降法.png)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# 定义损失函数求导函数\n",
    "def dJ(theta, X_b, y):\n",
    "    \"\"\"\n",
    "        损失函数求导函数定义\n",
    "    :theta 求导变量,关于theta求导\n",
    "    :X_b 数据集X新增一列,且值都为1的新矩阵\n",
    "    :y   数据集y\n",
    "    :return 损失函数求导函数\n",
    "    \"\"\"\n",
    "    result = np.empty(len(theta))  # 构建大小为len(theta)的空列表\n",
    "    result[0] = np.sum(X_b.dot(theta) - y)  # 先处理theta_0的导数\n",
    "    \n",
    "    # 对之后的所有theta进行求导,除了第一列theta_0\n",
    "    for i in range(1, len(theta)):\n",
    "        result[i] = (X_b.dot(theta) - y).dot(X_b[:, i])\n",
    "        \n",
    "    return 2 * result / len(X_b)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# 定义梯度下降函数\n",
    "def gradient_descent(X_b, y, initial_theta, i_iter=1e4, eta=0.01, epsilon=1e-8):\n",
    "    \"\"\"\n",
    "        梯度下降函数定义\n",
    "    :X_b 数据集X新增一列,且值都为1的新矩阵\n",
    "    :y   数据集y\n",
    "    :initial_theta   初始theta\n",
    "    :i_iter   迭代次数\n",
    "    :eta   学习率(步长),默认0.01\n",
    "    :epsilon   无限接近0的数\n",
    "    :return theta值\n",
    "    \"\"\"\n",
    "    theta = initial_theta\n",
    "    count = 0\n",
    "    while count < i_iter:\n",
    "        gradient = dJ(theta, X_b, y)\n",
    "        last_theta = theta\n",
    "        theta = theta - eta * gradient  # 不能写为theta -= eta * gradient\n",
    "        if abs(J(theta, X_b, y) - J(last_theta, X_b, y)) < epsilon:\n",
    "            break\n",
    "        count += 1\n",
    "    \n",
    "    return theta"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 不能写为theta -= eta * gradient的原因\n",
    "因为 theta = theta - eta * gradient，每次 theta - eta * gradient 产生了一个新的 np.array 的对象，赋给 theta，也就是让 theta 指向了一个全新的向量；\n",
    "\n",
    "但是，theta -= eta * gradient 只是修改当前 theta 指向的向量的内存。\n",
    "\n",
    "\n",
    "因为前面有一句 last_theta = theta，这将直接也修改 last_theta 的值。\n",
    "\n",
    "如果想使用 theta -= eta * gradient，就要让 last_theta 和 theta 的指向不一致。方法是，可以把 last_theta = theta 这一句改成：last_theta = np.array(theta)。\n",
    "\n",
    "即根据 theta 的值，创建一个新的 np.array，赋给 last_theta。\n",
    "\n",
    "**另外**,只有可变类型存在这种问题,不可变类型不存在,普通常量赋值使用 '-=' 操作没有任何问题\n",
    "\n",
    "其实,在Python中一切皆对象,theta就是np.array对象,可变类型"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "未做任何修改的lst1地址 1931448418016\n",
      "lst1 -= lst1 - lst2修改的lst1地址 1931448418016\n",
      "lst1 = lst1 - lst2修改的lst1地址 1931451776816\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# 验证-=操作符\n",
    "lst1 = np.array([1, 2, 3, 4])\n",
    "print('未做任何修改的lst1地址', id(lst1))\n",
    "lst2 = np.array([2, 3, 4, 5])\n",
    "\n",
    "lst1 -= lst1 - lst2  # 地址未变,还是刚开始定义lst1的地址\n",
    "print('lst1 -= lst1 - lst2修改的lst1地址', id(lst1))  \n",
    "\n",
    "lst1 = lst1 - lst2  # 地址变了,lst1指向了一个全新的地址\n",
    "print('lst1 = lst1 - lst2修改的lst1地址', id(lst1))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 验证 测试\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[1.        , 1.08680988],\n       [1.        , 0.55673877],\n       [1.        , 0.84903518],\n       [1.        , 1.68955226],\n       [1.        , 0.00943771],\n       [1.        , 0.24313824],\n       [1.        , 1.34149817],\n       [1.        , 1.65170551],\n       [1.        , 0.27341318],\n       [1.        , 1.15018666]])"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 7
    }
   ],
   "source": [
    "# 构造X_b数据集, 在X第一列新增值都为1的列\n",
    "X_b = np.hstack((np.ones((len(X),1)), X))\n",
    "X_b[:10]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "array([0., 0.])"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 8
    }
   ],
   "source": [
    "# 构造theta初始值, 假设从0开始  \n",
    "# X_b.shape[1] 是保证每个x都有系数theta\n",
    "initial_theta = np.zeros(X_b.shape[1])\n",
    "initial_theta"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "array([3.98255707, 2.86436851])"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 9
    }
   ],
   "source": [
    "# 使用梯度下降法计算theta值\n",
    "theta = gradient_descent(X_b, y, initial_theta)\n",
    "theta"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "使用自己封装的梯度下降法来训练线性回归模型"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "LinearRegression()"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 22
    }
   ],
   "source": [
    "from LinearRegression import LinearRegression\n",
    "\n",
    "\n",
    "estimator = LinearRegression()\n",
    "\n",
    "estimator.fit_gd(X, y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "3.9825570701953943"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 23
    }
   ],
   "source": [
    "# 查看截距\n",
    "estimator.intercept_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "array([2.86436851])"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 24
    }
   ],
   "source": [
    "# 查看系数\n",
    "estimator.coef_\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}